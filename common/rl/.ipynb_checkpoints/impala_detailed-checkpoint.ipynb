{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# IMPALA\n",
    "\n",
    "Basic overview\n",
    "- Actor critic\n",
    "- Multiple actors, one critic?\n",
    "- V-trace is used to weight information from trajectories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actor-Learner architecture\n",
    "We have a set of actors, and one or more learners. The actors generate trajectories, and the learners learn a policy (off-policy)\n",
    "\n",
    "### Actors\n",
    "Overview\n",
    "- At the start of the trajectory, the actor updates its parameters to match the current learner parameters\n",
    "- It runs for $n$ steps\n",
    "- It sends the trajectory (states, actions, rewards) to the learner, and the policy distributions\n",
    "\n",
    "### Learner\n",
    "- The learner receives a trajectory\n",
    "- It updates its parameters based on the trajectory (HOW?)\n",
    "- We can also use multiple learners distributed over many GPUs (using standard distributed ML techniques)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V-trace\n",
    "Consider a trajectory $t = (a_i, x_i, r_i)$. The V-trace target is:\n",
    "$$\n",
    "v_s = V(x_s) + \\sigma_{t=s}^{s + n - 1} \\gamma ^{t-s} \\prod_{i=s}^{t-1} c_i \\delta_t V\n",
    "$$\n",
    "where $\\delta_t V$ is a temporal difference for $V$:\n",
    "$$\n",
    "\\delta_t V = \\rho_t (r_t + \\gamma V(x_{t+1}) - V(x_t))\n",
    "$$\n",
    "Here, $V$ is the value function approximation. $\\rho_t$ and $c_i$ are both importance sampling constants:\n",
    "$$\n",
    "\\rho_t = \\min (\\bar{\\rho}, \\frac{\\pi(a_t, x_t)}{\\mu(a_t, x_t)})\n",
    "$$\n",
    "and\n",
    "$$\n",
    "c_i  = \\min (\\bar{c}, \\frac{\\pi(a_i, x_i)}{\\mu(a_i, x_i)})\n",
    "$$\n",
    "Here $\\pi$ is the current policy of the learner, and $\\mu$ is the policy followed by the actor that generated the trajectory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actor-Critic version\n",
    "We update our value parameters with:\n",
    "$$\n",
    "v_s - V(x_s)\\nabla V(x_s)\n",
    "$$\n",
    "We update our policy parameters with:\n",
    "$$\n",
    "\\rho \\nabla \\log \\pi(a_s, x_s) (r_s + \\gamma v_{s+1} - V(x_s))\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}