{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SARSA\n",
    "SARSA is on policy TD control (in contrast to Q learning, which is off policy TD control).\n",
    "\n",
    "- On policy: actions are chosen using the actual policy\n",
    "- TD control: the value predictor is updated at every time step"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trajectory(q_pred, policy, env):\n",
    "    state, _, done, _ = env.reset()\n",
    "    action_values = q_pred(state)\n",
    "    action = policy(action_values)\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action_values = q_pred(next_state)\n",
    "        next_action = policy(next_action_values)\n",
    "        q_loss = alpha * (reward + gamma * next_action_values[next_action] - q_pred(state)[action])\n",
    "        q_loss.backwards()\n",
    "        optimizer.step()\n",
    "        state = next_state\n",
    "        action = next_action\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}