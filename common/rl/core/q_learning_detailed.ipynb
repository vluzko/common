{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Q-Learning\n",
    "Q-learning is off policy TD control (in contrast to SARSA, which is on policy TD control)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_trajectory(q_pred, policy, env, optimizer):\n",
    "    state, _, done, _ = env.reset()\n",
    "    while not done:\n",
    "        action_values = q_pred(state)\n",
    "        action = policy(action_values)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # The max here is what makes it off policy\n",
    "        q_loss = reward + gamma * q_pred(next_state).max() - q_pred(state)[action]\n",
    "        q_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return q_pred"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}