{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRPO and PPO\n",
    "In the normal policy gradient, we update with (or can update with):\n",
    "$$\n",
    "L(\\theta) = E[\\log \\pi_\\theta(a[t], s[t]) \\hat{A}(s[t])]\n",
    "$$\n",
    "where $\\hat{A}$ is an estimate of the advantage function.\n",
    "\n",
    "In TRPO and PPO, we try to limit the size of some of our updates based on some measure of the uncertainty in the update.\n",
    "\n",
    "Define $r_t(\\theta)$ as:\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a[t], s[t])}{\\pi_{\\theta_o}(a[t], s[t])}\n",
    "$$\n",
    "i.e. the ratio of probabilities the current parameters give to the probabilities some previous parameters give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRPO\n",
    "In TRPO, we first pick a step size $\\delta$.\n",
    "\n",
    "We also use importance sampling to enable us to use some off-policy data:\n",
    "$$\n",
    "\\nabla_{\\theta^\\prime} J = E_{\\tau \\sim \\pi_\\theta}[\\sum_t \\nabla_{\\theta^\\prime} \\log \\pi_{\\theta^\\prime}(a[t], s[t]) I(a[t], s[t]) (\\sum_{t^\\prime = t} r(a[t^\\prime], s[t^\\prime]))]\n",
    "$$\n",
    "here $I(a[t], s[t])$ is the importance sampling:\n",
    "$$\n",
    "I(a[t], s[t]) = \\prod_{i = 1}^t \\frac{\\pi_{\\theta^\\prime}(a[i], s[i])}{\\pi_\\theta(a[i], s[i])}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
